{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"slrwithgradientdescentandscikit-200924-150406.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"PZWh679fDLMH"},"source":["\n","# Simple Linear Regression with Gradient Descent and Scikit-Learn\n","\n","\n","The purpose of this notebook is to explain the implementation of a simple linear regression model with two different approaches. The first is with the gradient descent algorithm. Second is using linear regression model from the Scikit-Learn module from Python library.\n","This notebook is also the continuation of our previous lab activity of implementing simple linear regression using ordinary least square method.\n","I have explained the general intuition and mathematical theory behind the gradient descent algorithm in my Youtube [video](https://youtu.be/T-ExXMXQxF0)."]},{"cell_type":"markdown","metadata":{"id":"BK2Nv7e-DLMW"},"source":["In the initialization phase, all the required Python libraries are imported, and the data set has been loaded as the data frame in the memory for further processing. \n","\n","Also, the data format is analyzed, and the relationship between the dependent and independent variable is visualized to develop an initial understanding."]},{"cell_type":"code","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"RAEnhYxwDLMn"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"id":"8xKkFxXTDLNV"},"source":["df = pd.read_csv('../input/predictingese/AttendanceMarksSA.csv')\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"5qub3isQDLNr"},"source":["X= df['MSE']\n","Y=df['ESE']\n","sns.scatterplot(X,Y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"DH6mRXJqDLOD"},"source":["beta0=0\n","beta1=0\n","alpha=0.01\n","count =10000\n","n=float(len(X))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BVq99xCgDLOU"},"source":["The code segment implements the iterative process of gradient descent algorithm. This code segment calculates the partial derivative of the error function. It minimizes the error function to calculate the values of beta0 and beta1."]},{"cell_type":"code","metadata":{"trusted":true,"id":"qUqy7iXcDLOX"},"source":["for i in range(count): \n","    Ybar = beta1*X + beta0    \n","    beta1 = beta1 - (alpha/n)*sum(X*(Ybar-Y))\n","    beta0 = beta0 - (alpha/n)*sum(Ybar-Y)\n","    \n","print(beta0,beta1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QiOQD8KFDLOw"},"source":["The following code segment plots the identified best fit line or regression line visually."]},{"cell_type":"code","metadata":{"trusted":true,"id":"L6q9DE2sDLO0"},"source":["Ybar = beta1*X + beta0\n","\n","plt.scatter(X, Y) \n","plt.plot([min(X), max(X)], [min(Ybar), max(Ybar)], color='red')  # regression line\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U6PhDB5ZDLPL"},"source":["We are reusing the same error caluclation RSE to calculate the Residual Standard Error."]},{"cell_type":"code","metadata":{"trusted":true,"id":"AdapjxlTDLPZ"},"source":["import math\n","def RSE(y_true, y_predicted):\n","   \n","    y_true = np.array(y_true)\n","    y_predicted = np.array(y_predicted)\n","    RSS = np.sum(np.square(y_true - y_predicted))\n","\n","    rse = math.sqrt(RSS / (len(y_true) - 2))\n","    return rse\n","\n","\n","rse= RSE(df['ESE'],Ybar)\n","print(rse)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UjVkYO5kDLPo"},"source":["The second way to indirectly implement the gradient descent algorithm is to use the LinearRegression module form Scikit-Learn. The SciKit-Learn use Object-Oriented approach to implementing various machine learning algorithms. \n","\n","Here, I am using LinearRegression() class and the fit() method from the LinearRegression() class assuming it implements the gradient descent algorithm."]},{"cell_type":"code","metadata":{"trusted":true,"id":"j20GzCYSDLPr"},"source":["from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n"," "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yIbjd9VJDLP9"},"source":["The following code segments first extract input and output feature vector from the data frame and convert them into the array representation."]},{"cell_type":"code","metadata":{"trusted":true,"id":"ZNwAn3p4DLQB"},"source":["x = np.array(df['MSE']).reshape(-1,1)\n","y = np.array(df['ESE']).reshape(-1,1)\n"," \n","\n","lr = LinearRegression()\n","lr.fit(x,y)\n","\n","\n","print(lr.coef_)\n","print(lr.intercept_)\n","\n","yp = lr.predict(x)\n","rse = RSE(y,yp)\n","\n","print(rse)\n","\n"," \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"trusted":true,"id":"D7T55S8QDLQW"},"source":["x = np.array(df['MSE']).reshape(-1,1)\n","y = np.array(df['ESE']).reshape(-1,1)\n"," \n","\n","lr = LinearRegression()\n","lr.fit(x,y)\n","\n","\n","print(lr.coef_)\n","print(lr.intercept_)\n","\n","yp = lr.predict(x)\n","rse = RSE(y,yp)\n","\n","print(rse)\n","\n"," \n"]},{"cell_type":"markdown","metadata":{"id":"eIfvQAm4DLQd"},"source":["**Interpretation of the Result:**\n","\n","The Linear Regression model implementd through the gradient descent algorithm  and from the LinearRegression() class from the Scikit-Learn module approximately claculates the same values for slope (beta1) and y-intercept(beta0) and also with the acceptable level of RSE i.e. **4.39.** i.e. the model predicts  end sem exam marks with +/- 4 error from mid-sem exam."]}]}